# ğŸš€ Transformer'larÄ± Cebe SÄ±ÄŸdÄ±rmak: Edge Cihazlar Ä°Ã§in Optimizasyon

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/transformer-edge-optimization)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

BÃ¼yÃ¼k Transformer modellerini mobil ve uÃ§ cihazlarda Ã§alÄ±ÅŸtÄ±rmak iÃ§in kapsamlÄ± rehber ve pratik Ã¶rnekler.

## ğŸ“‹ Ä°Ã§indekiler

- [Genel BakÄ±ÅŸ](#genel-bakÄ±ÅŸ)
- [Kurulum](#kurulum)
- [Notebook'lar](#notebooks)
- [Optimizasyon Teknikleri](#optimizasyon-teknikleri)
- [Pratik Ã–rnekler](#pratik-Ã¶rnekler)
- [Benchmark SonuÃ§larÄ±](#benchmark-sonuÃ§larÄ±)
- [Kaynaklar](#kaynaklar)

## ğŸ¯ Genel BakÄ±ÅŸ

Bu repo, Transformer modellerinin mobil telefonlar, IoT cihazlarÄ± ve edge computing platformlarÄ±nda verimli Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± iÃ§in gerekli teknikleri ve araÃ§larÄ± iÃ§erir.

### Kapsanan Konular

- **Quantization**: INT8, FP16, Dynamic Quantization
- **Knowledge Distillation**: DistilBERT, TinyBERT, MobileBERT
- **Pruning**: Structured ve Unstructured Pruning
- **AraÃ§lar**: Hugging Face Optimum, ONNX Runtime, TensorFlow Lite, Core ML
- **Deployment**: Android, iOS, Web uygulamalarÄ±

## ğŸ”§ Kurulum

### Temel BaÄŸÄ±mlÄ±lÄ±klar

```bash
# PyTorch ve Transformers
pip install torch transformers

# Optimizasyon araÃ§larÄ±
pip install optimum[onnxruntime] onnx onnxruntime

# TensorFlow Lite (isteÄŸe baÄŸlÄ±)
pip install tensorflow

# Quantization araÃ§larÄ±
pip install neural-compressor
```

### Repo'yu Klonlama

```bash
git clone https://github.com/yourusername/transformer-edge-optimization.git
cd transformer-edge-optimization
pip install -r requirements.txt
```

## ğŸ““ Notebook'lar

### 1. Quantization
- **[01_quantization_basics.ipynb](notebooks/01_quantization_basics.ipynb)** - Quantization temelleri ve PyTorch Ã¶rnekleri
- **[02_huggingface_optimum.ipynb](notebooks/02_huggingface_optimum.ipynb)** - Hugging Face Optimum ile INT8 quantization
- **[03_dynamic_quantization.ipynb](notebooks/03_dynamic_quantization.ipynb)** - Dynamic quantization BERT Ã¶rneÄŸi

### 2. Knowledge Distillation
- **[04_distillation_basics.ipynb](notebooks/04_distillation_basics.ipynb)** - Knowledge distillation temel prensipler
- **[05_distilbert_training.ipynb](notebooks/05_distilbert_training.ipynb)** - DistilBERT'ten Ã¶ÄŸrenci model eÄŸitimi

### 3. Pruning
- **[06_pruning_techniques.ipynb](notebooks/06_pruning_techniques.ipynb)** - Magnitude ve structured pruning
- **[07_attention_head_pruning.ipynb](notebooks/07_attention_head_pruning.ipynb)** - BERT attention head pruning

### 4. Model DÃ¶nÃ¼ÅŸÃ¼mleri
- **[08_pytorch_to_onnx.ipynb](notebooks/08_pytorch_to_onnx.ipynb)** - PyTorch â†’ ONNX dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- **[09_tensorflow_lite.ipynb](notebooks/09_tensorflow_lite.ipynb)** - TensorFlow Lite dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- **[10_coreml_conversion.ipynb](notebooks/10_coreml_conversion.ipynb)** - Core ML dÃ¶nÃ¼ÅŸÃ¼mÃ¼

### 5. Deployment
- **[11_android_tflite.ipynb](notebooks/11_android_tflite.ipynb)** - Android TFLite deployment
- **[12_transformers_js.ipynb](notebooks/12_transformers_js.ipynb)** - TarayÄ±cÄ±da Transformers.js

### 6. Benchmark & KarÅŸÄ±laÅŸtÄ±rma
- **[13_benchmarking.ipynb](notebooks/13_benchmarking.ipynb)** - Performans karÅŸÄ±laÅŸtÄ±rmalarÄ±
- **[14_end_to_end_pipeline.ipynb](notebooks/14_end_to_end_pipeline.ipynb)** - Tam optimizasyon pipeline'Ä±

## ğŸ”¬ Optimizasyon Teknikleri

### Quantization

```python
import torch
from transformers import BertForSequenceClassification

# Model yÃ¼kle
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Dynamic quantization
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)
```

### Knowledge Distillation

```python
def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)
    
    hard_loss = F.cross_entropy(student_logits, labels)
    return alpha * soft_loss + (1 - alpha) * hard_loss
```

### Pruning

```python
import torch.nn.utils.prune as prune

# %30 magnitude pruning
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)
        prune.remove(module, 'weight')
```

## ğŸ’¡ Pratik Ã–rnekler

### Android Sentiment Analysis

```kotlin
class SentimentAnalyzer(context: Context) {
    private val interpreter: Interpreter
    
    fun predict(text: String): Float {
        val inputIds = tokenize(text)
        val output = Array(1) { FloatArray(2) }
        interpreter.run(inputIds, output)
        return output[0][1] // Positive score
    }
}
```

### Web TarayÄ±cÄ±da NER

```javascript
import { pipeline } from '@xenova/transformers';

const ner = await pipeline('ner', 'Xenova/bert-base-NER');
const entities = await ner('Apple is looking at buying UK startup');
console.log(entities);
```

## ğŸ“Š Benchmark SonuÃ§larÄ±

| Model | Boyut | Inferans SÃ¼resi (ms) | DoÄŸruluk |
|-------|-------|----------------------|----------|
| BERT-base (FP32) | 440 MB | 350 ms | 92.5% |
| DistilBERT (FP32) | 255 MB | 220 ms | 89.8% |
| DistilBERT (INT8) | 67 MB | 95 ms | 88.2% |
| TinyBERT (INT8) | 14 MB | 37 ms | 87.1% |

*Benchmark: Pixel 6, 128 token input*

## ğŸ› ï¸ AraÃ§lar ve Framework'ler

### Desteklenen AraÃ§lar

- **Hugging Face Optimum** - Hardware-accelerated inference
- **ONNX Runtime** - Cross-platform optimization
- **TensorFlow Lite** - Mobile deployment
- **Core ML** - iOS optimization
- **Transformers.js** - Browser inference
- **Intel Neural Compressor** - Intel CPU quantization

## ğŸ“š Kaynaklar

### Akademik Makaleler

- [DistilBERT (Sanh et al., 2019)](https://arxiv.org/abs/1910.01108)
- [TinyBERT (Jiao et al., 2020)](https://arxiv.org/abs/1909.10351)
- [MobileBERT (Sun et al., 2020)](https://arxiv.org/abs/2004.02984)
- [Q8BERT (Zafrir et al., 2021)](https://arxiv.org/abs/1910.06188)

### DokÃ¼mantasyon

- [Hugging Face Optimum](https://huggingface.co/docs/optimum)
- [ONNX Runtime](https://onnxruntime.ai)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite)
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)

### Topluluk

- [Hugging Face Forums](https://discuss.huggingface.co)
- [Reddit r/MachineLearning](https://reddit.com/r/MachineLearning)

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±larÄ±nÄ±zÄ± bekliyoruz! LÃ¼tfen ÅŸu adÄ±mlarÄ± izleyin:

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

## ğŸ™ TeÅŸekkÃ¼rler

- Hugging Face ekibine harika araÃ§lar iÃ§in
- Anthropic'e Claude iÃ§in
- AÃ§Ä±k kaynak topluluÄŸuna

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilir veya iletiÅŸime geÃ§ebilirsiniz.

---

â­ Projeyi beÄŸendiyseniz yÄ±ldÄ±z vermeyi unutmayÄ±n!
