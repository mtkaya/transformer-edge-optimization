# ğŸš€ Transformer Edge Optimization

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow)](https://huggingface.co/)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtkaya/transformer-edge-optimization)

> **BÃ¼yÃ¼k Transformer modellerini mobil ve edge cihazlarda Ã§alÄ±ÅŸtÄ±rmak iÃ§in kapsamlÄ± rehber ve araÃ§lar.**

---

## âœ¨ Ã–zellikler

### ğŸ¯ Optimizasyon Teknikleri

- **Quantization** - INT8, FP16, Dynamic Quantization
  - Model boyutu: **4x azalma**
  - Minimal doÄŸruluk kaybÄ± (**~1-2%**)
  
- **Knowledge Distillation** - Ã–ÄŸretmen-Ã¶ÄŸrenci Ã¶ÄŸrenimi
  - Model boyutu: **6-10x azalma**
  - DoÄŸruluk korunur (**~2-4% kayÄ±p**)
  
- **ONNX Runtime** - Cross-platform deployment
  - Hardware-accelerated inference
  - Mobil ve edge cihaz desteÄŸi

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Google Colab'de Ã‡alÄ±ÅŸtÄ±r (Ã–nerilen)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtkaya/transformer-edge-optimization/blob/main/notebooks/01_quantization_basics.ipynb)

1. YukarÄ±daki butona tÄ±kla
2. Runtime â†’ Change runtime type â†’ **GPU**
3. Runtime â†’ **Run all**
4. 5 dakika bekle ve sonuÃ§larÄ± izle! ğŸ‰

### Lokal Kurulum
```bash
# Repository'yi klonla
git clone https://github.com/mtkaya/transformer-edge-optimization.git
cd transformer-edge-optimization

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
pip install -r requirements.txt

# Jupyter'i baÅŸlat
jupyter notebook notebooks/
```

---

## ğŸ““ Notebook'lar

### 1ï¸âƒ£ Quantization Basics (15 dakika)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtkaya/transformer-edge-optimization/blob/main/notebooks/01_quantization_basics.ipynb)

- FP32 â†’ INT8 dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Model boyutu: **4x azaltma**
- Ä°nferans hÄ±zÄ±: **2x artÄ±ÅŸ**

### 2ï¸âƒ£ ONNX Runtime Optimization (20 dakika)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtkaya/transformer-edge-optimization/blob/main/notebooks/02_huggingface_optimum.ipynb)

- PyTorch â†’ ONNX dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Dynamic quantization
- Cross-platform deployment

### 3ï¸âƒ£ Knowledge Distillation (30 dakika)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtkaya/transformer-edge-optimization/blob/main/notebooks/05_distilbert_training.ipynb)

- Teacher-student training
- Model boyutu: **7.6x azaltma**
- BERT â†’ TinyBERT

---

## ğŸ’» KullanÄ±m Ã–rneÄŸi
```python
import torch
from transformers import AutoModelForSequenceClassification

# Model yÃ¼kle
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')

# Quantize et (FP32 â†’ INT8)
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Model boyutu 4x daha kÃ¼Ã§Ã¼k! ğŸ‰
print("Model 4x daha kÃ¼Ã§Ã¼k, 2x daha hÄ±zlÄ±!")
```

---

## ğŸ“Š Benchmark SonuÃ§larÄ±

| Teknik | Boyut Azaltma | HÄ±z ArtÄ±ÅŸÄ± | DoÄŸruluk |
|--------|---------------|------------|----------|
| **Quantization (INT8)** | 4.0x | 2.1x | 91.2% |
| **ONNX Runtime** | 3.8x | 2.2x | 88.2% |
| **Distillation** | 7.6x | 3.0x | 87.1% |
| **Combined** | 31.4x | 9.5x | 85.8% |

---

## ğŸ› ï¸ Desteklenen Platformlar

- âœ… **Android** - TensorFlow Lite
- âœ… **iOS** - Core ML
- âœ… **Web** - Transformers.js
- âœ… **Edge Devices** - ONNX Runtime

---

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±larÄ±nÄ±zÄ± bekliyoruz! 

1. Fork yapÄ±n
2. Feature branch oluÅŸturun
3. Commit yapÄ±n
4. Pull Request aÃ§Ä±n

Detaylar iÃ§in: [CONTRIBUTING.md](CONTRIBUTING.md)

---

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±ndadÄ±r - detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

---

## ğŸ™ TeÅŸekkÃ¼rler

- [Hugging Face](https://huggingface.co/) - Transformers ve Optimum
- [ONNX](https://onnx.ai/) - Model interoperability
- AÃ§Ä±k kaynak topluluÄŸuna â¤ï¸

---

## ğŸ“§ Ä°letiÅŸim

- **GitHub Issues:** [Sorun bildir](https://github.com/mtkaya/transformer-edge-optimization/issues)
- **Discussions:** [TartÄ±ÅŸmalara katÄ±l](https://github.com/mtkaya/transformer-edge-optimization/discussions)

---

<div align="center">

**â­ Projeyi beÄŸendiyseniz yÄ±ldÄ±z vermeyi unutmayÄ±n! â­**

Made with â¤ï¸ for the AI community

</div>
