{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Optimum ile INT8 Quantization\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/transformer-edge-optimization/blob/main/notebooks/02_huggingface_optimum.ipynb)\n",
    "\n",
    "Hugging Face Optimum kullanarak ONNX Runtime ile model optimizasyonu.\n",
    "\n",
    "## İçerik\n",
    "1. Optimum kurulumu\n",
    "2. Model ONNX'e dönüştürme\n",
    "3. Static quantization\n",
    "4. ONNX Runtime inference\n",
    "5. Performans karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli paketleri yükle\n",
    "!pip install -q optimum[onnxruntime] onnx onnxruntime transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model ve Tokenizer Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# PyTorch modelini ONNX'e dönüştürerek yükle\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    export=True  # ONNX'e dönüştür\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded and converted to ONNX: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibration Dataset Hazırlama\n",
    "\n",
    "Static quantization için kalibrasyon verisi gereklidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST-2 dataset'ten sample al\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:100]\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "calibration_dataset = dataset.map(preprocess_function, batched=True)\n",
    "calibration_dataset = calibration_dataset.remove_columns([\"sentence\", \"label\", \"idx\"])\n",
    "\n",
    "print(f\"Calibration dataset size: {len(calibration_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization Config Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static quantization config (INT8)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "    is_static=True,\n",
    "    per_channel=True\n",
    ")\n",
    "\n",
    "print(\"Quantization config:\")\n",
    "print(f\"- Type: Static INT8\")\n",
    "print(f\"- Per-channel: True\")\n",
    "print(f\"- Backend: AVX512_VNNI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantizer oluştur\n",
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "\n",
    "# Quantize et\n",
    "quantizer.quantize(\n",
    "    save_dir=\"./distilbert_quantized\",\n",
    "    quantization_config=qconfig,\n",
    "    calibration_dataset=calibration_dataset\n",
    ")\n",
    "\n",
    "print(\"Quantization completed!\")\n",
    "print(\"Saved to: ./distilbert_quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantized Model Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized modeli yükle\n",
    "quantized_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"./distilbert_quantized\"\n",
    ")\n",
    "\n",
    "print(\"Quantized model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performans Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_texts = [\n",
    "    \"This movie was absolutely wonderful!\",\n",
    "    \"I hated every second of it.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"Best film I've seen this year!\",\n",
    "    \"Terrible waste of time and money.\"\n",
    "]\n",
    "\n",
    "def benchmark_model(model, texts, num_runs=50):\n",
    "    times = []\n",
    "    \n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Warm-up\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(**inputs)\n",
    "        elapsed = (time.time() - start) / num_runs * 1000  # ms\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Benchmark original\n",
    "orig_mean, orig_std = benchmark_model(model, test_texts)\n",
    "print(f\"Original model: {orig_mean:.2f} ± {orig_std:.2f} ms\")\n",
    "\n",
    "# Benchmark quantized\n",
    "quant_mean, quant_std = benchmark_model(quantized_model, test_texts)\n",
    "print(f\"Quantized model: {quant_mean:.2f} ± {quant_std:.2f} ms\")\n",
    "\n",
    "speedup = orig_mean / quant_mean\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Doğruluk Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:500]\")\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example[\"sentence\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        pred = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        labels.append(example[\"label\"])\n",
    "    \n",
    "    return accuracy_score(labels, predictions)\n",
    "\n",
    "orig_acc = evaluate_model(model, test_dataset)\n",
    "quant_acc = evaluate_model(quantized_model, test_dataset)\n",
    "\n",
    "print(f\"Original model accuracy: {orig_acc*100:.2f}%\")\n",
    "print(f\"Quantized model accuracy: {quant_acc*100:.2f}%\")\n",
    "print(f\"Accuracy drop: {(orig_acc - quant_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Boyutu Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_size(path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # MB\n",
    "\n",
    "# Not: Original ONNX model path'i kontrol edin\n",
    "# original_size = get_directory_size(\"./ort_model\")\n",
    "quantized_size = get_directory_size(\"./distilbert_quantized\")\n",
    "\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "# print(f\"Compression ratio: {original_size/quantized_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Özet\n",
    "\n",
    "Hugging Face Optimum ile:\n",
    "- ✅ PyTorch → ONNX dönüşümü\n",
    "- ✅ Static INT8 quantization\n",
    "- ✅ Hızlı ONNX Runtime inference\n",
    "- ✅ Minimal doğruluk kaybı\n",
    "- ✅ Önemli hız artışı\n",
    "\n",
    "### Avantajlar\n",
    "- Hardware-agnostic optimizasyon\n",
    "- Cross-platform deployment\n",
    "- Production-ready araçlar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
