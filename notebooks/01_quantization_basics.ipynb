{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Temelleri\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/transformer-edge-optimization/blob/main/notebooks/01_quantization_basics.ipynb)\n",
    "\n",
    "Bu notebook'ta quantization (niceleme) temellerini öğreneceğiz ve BERT modeline dynamic quantization uygulayacağız.\n",
    "\n",
    "## İçerik\n",
    "1. Quantization nedir?\n",
    "2. FP32 → INT8 dönüşümü\n",
    "3. PyTorch Dynamic Quantization\n",
    "4. Model boyutu karşılaştırması\n",
    "5. Performans analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri yükle\n",
    "!pip install -q torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantization Nedir?\n",
    "\n",
    "Quantization, yüksek hassasiyetli sayıları (FP32) daha düşük hassasiyette (INT8) temsil ederek:\n",
    "- Model boyutunu azaltır (4x)\n",
    "- İnferans hızını artırır\n",
    "- Bellek kullanımını düşürür\n",
    "\n",
    "### Quantization Formülü\n",
    "\n",
    "```\n",
    "quantized = round(float_value / scale) + zero_point\n",
    "dequantized = (quantized - zero_point) * scale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit quantization örneği\n",
    "def quantize_tensor(tensor, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = 2**num_bits - 1\n",
    "    \n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "    \n",
    "    q_tensor = torch.round(tensor / scale + zero_point)\n",
    "    q_tensor = torch.clamp(q_tensor, qmin, qmax)\n",
    "    \n",
    "    return q_tensor.to(torch.uint8), scale, zero_point\n",
    "\n",
    "# Test\n",
    "original = torch.randn(5, 5)\n",
    "quantized, scale, zero_point = quantize_tensor(original)\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(original)\n",
    "print(f\"\\nQuantized tensor (INT8):\")\n",
    "print(quantized)\n",
    "print(f\"\\nScale: {scale:.6f}, Zero point: {zero_point:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Modeli Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT modelini yükle\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model evaluation moduna al\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Quantization Uygulama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},  # Sadece Linear katmanları quantize et\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"Quantization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Boyutu Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model, filename=\"temp_model.pt\"):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    os.remove(filename)\n",
    "    return size_mb\n",
    "\n",
    "# Boyutları hesapla\n",
    "original_size = get_model_size(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "compression_ratio = original_size / quantized_size\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. İnferans Hızı Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input'u hazırla\n",
    "test_text = \"This movie is absolutely fantastic! I loved every minute of it.\"\n",
    "inputs = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "    _ = quantized_model(**inputs)\n",
    "\n",
    "# Benchmark original model\n",
    "num_iterations = 100\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iterations):\n",
    "        _ = model(**inputs)\n",
    "original_time = (time.time() - start) / num_iterations * 1000  # ms\n",
    "\n",
    "# Benchmark quantized model\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iterations):\n",
    "        _ = quantized_model(**inputs)\n",
    "quantized_time = (time.time() - start) / num_iterations * 1000  # ms\n",
    "\n",
    "speedup = original_time / quantized_time\n",
    "\n",
    "print(f\"Original model: {original_time:.2f} ms\")\n",
    "print(f\"Quantized model: {quantized_time:.2f} ms\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tahmin Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cümleleri\n",
    "test_sentences = [\n",
    "    \"This is an amazing product!\",\n",
    "    \"I'm very disappointed with the service.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Absolutely terrible experience.\",\n",
    "    \"Best purchase I've ever made!\"\n",
    "]\n",
    "\n",
    "print(\"Predictions comparison:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        original_output = model(**inputs).logits\n",
    "        quantized_output = quantized_model(**inputs).logits\n",
    "    \n",
    "    orig_pred = torch.argmax(original_output, dim=1).item()\n",
    "    quant_pred = torch.argmax(quantized_output, dim=1).item()\n",
    "    \n",
    "    label = \"POSITIVE\" if orig_pred == 1 else \"NEGATIVE\"\n",
    "    match = \"✓\" if orig_pred == quant_pred else \"✗\"\n",
    "    \n",
    "    print(f\"{match} {sentence[:50]}...\")\n",
    "    print(f\"  Original: {label}, Quantized: {'POSITIVE' if quant_pred == 1 else 'NEGATIVE'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Özet\n",
    "\n",
    "Bu notebook'ta:\n",
    "- ✅ Quantization temellerini öğrendik\n",
    "- ✅ BERT modeline dynamic quantization uyguladık\n",
    "- ✅ Model boyutunu ~4x azalttık\n",
    "- ✅ İnferans hızını artırdık\n",
    "- ✅ Minimal doğruluk kaybı gözlemledik\n",
    "\n",
    "### Sonraki Adımlar\n",
    "- INT8 static quantization\n",
    "- Quantization-aware training\n",
    "- ONNX Runtime ile quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized modeli kaydet\n",
    "torch.save(quantized_model.state_dict(), 'bert_quantized_int8.pt')\n",
    "print(\"Quantized model saved: bert_quantized_int8.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
